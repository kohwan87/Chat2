{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_bert import load_trained_model_from_checkpoint\n",
    "from keras.layers import Layer\n",
    "from keras_bert import Tokenizer\n",
    "import keras as keras\n",
    "from keras import backend as K\n",
    "import os\n",
    "import codecs\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 384\n",
    "\n",
    "config_path = os.path.join('bert', 'bert_config.json')       \n",
    "checkpoint_path = os.path.join('bert', 'bert_model.ckpt')    \n",
    "vocab_path = os.path.join('bert', 'vocab.txt')\n",
    "\n",
    "model = load_trained_model_from_checkpoint(config_path, checkpoint_path, training=False, trainable=True, seq_len=SEQ_LEN,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonMasking(Layer):   \n",
    "    def __init__(self, **kwargs):   \n",
    "        self.supports_masking = True  \n",
    "        super(NonMasking, self).__init__(**kwargs)   \n",
    "  \n",
    "    def build(self, input_shape):   \n",
    "        input_shape = input_shape   \n",
    "  \n",
    "    def compute_mask(self, input, input_mask=None):   \n",
    "        return None   \n",
    "  \n",
    "    def call(self, x, mask=None):   \n",
    "        return x   \n",
    "  \n",
    "    def get_output_shape_for(self, input_shape):   \n",
    "        return input_shape\n",
    "    \n",
    "class MyLayer_Start(Layer):\n",
    "\n",
    "    def __init__(self,seq_len, **kwargs):\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.supports_masking = True\n",
    "        super(MyLayer_Start, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        self.W = self.add_weight(name='kernel', \n",
    "                                 shape=(input_shape[2],2),\n",
    "                                 initializer='uniform',\n",
    "                                 trainable=True)\n",
    "        super(MyLayer_Start, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        x = K.reshape(x, shape=(-1,self.seq_len,K.shape(x)[2]))\n",
    "        x = K.dot(x, self.W)\n",
    "        \n",
    "        x = K.permute_dimensions(x, (2,0,1))\n",
    "\n",
    "        self.start_logits, self.end_logits = x[0], x[1]\n",
    "        \n",
    "        self.start_logits = K.softmax(self.start_logits, axis=-1)\n",
    "        \n",
    "        return self.start_logits\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.seq_len)\n",
    "\n",
    "\n",
    "class MyLayer_End(Layer):\n",
    "    \n",
    "    def __init__(self,seq_len, **kwargs):\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.supports_masking = True\n",
    "        super(MyLayer_End, self).__init__(**kwargs)\n",
    "  \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        self.W = self.add_weight(name='kernel', \n",
    "                                 shape=(input_shape[2], 2),\n",
    "                                 initializer='uniform',\n",
    "                                 trainable=True)\n",
    "        super(MyLayer_End, self).build(input_shape)\n",
    "\n",
    "  \n",
    "    def call(self, x):\n",
    "\n",
    "        \n",
    "        x = K.reshape(x, shape=(-1,self.seq_len,K.shape(x)[2]))\n",
    "        x = K.dot(x, self.W)\n",
    "        x = K.permute_dimensions(x, (2,0,1))\n",
    "        \n",
    "        self.start_logits, self.end_logits = x[0], x[1]\n",
    "        \n",
    "        self.end_logits = K.softmax(self.end_logits, axis=-1)\n",
    "        \n",
    "        return self.end_logits\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_finetuning_model(model):\n",
    "    \n",
    "    inputs = model.inputs[:2]\n",
    "    dense = model.output\n",
    "    x = NonMasking()(dense)\n",
    "    outputs_start = MyLayer_Start(SEQ_LEN)(x)\n",
    "    outputs_end = MyLayer_End(SEQ_LEN)(x)\n",
    "    bert_model = keras.models.Model(inputs, [outputs_start, outputs_end])\n",
    "  \n",
    "    return bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = get_bert_finetuning_model(model)\n",
    "bert_model.load_weights(\"korquad_3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict = {}\n",
    "with codecs.open(vocab_path, 'r', 'utf8') as reader:\n",
    "    for line in reader:\n",
    "        token = line.strip()\n",
    "        if \"_\" in token:\n",
    "            token = token.replace(\"_\",\"\")\n",
    "            token = \"##\" + token\n",
    "        token_dict[token] = len(token_dict)\n",
    "\n",
    "reverse_token_dict = {v : k for k, v in token_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class inherit_Tokenizer(Tokenizer):\n",
    "    def _tokenize(self, text):\n",
    "        if not self._cased:\n",
    "            text = text\n",
    "            \n",
    "            text = text.lower()\n",
    "        spaced = ''\n",
    "        for ch in text:\n",
    "            if self._is_punctuation(ch) or self._is_cjk_character(ch):\n",
    "                spaced += ' ' + ch + ' '\n",
    "            elif self._is_space(ch):\n",
    "                spaced += ' '\n",
    "            elif ord(ch) == 0 or ord(ch) == 0xfffd or self._is_control(ch):\n",
    "                continue\n",
    "            else:\n",
    "                spaced += ch\n",
    "        tokens = []\n",
    "        for word in spaced.strip().split():\n",
    "            tokens += self._word_piece_tokenize(word)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = inherit_Tokenizer(token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## question과 paragraph를 받아서, token, segment를 만든다.\n",
    "\n",
    "def convert_pred_data(question, doc):\n",
    "    global tokenizer\n",
    "    indices, segments = [], []\n",
    "    ids, segment = tokenizer.encode(question, doc, max_len=SEQ_LEN)\n",
    "    indices.append(ids)\n",
    "    segments.append(segment)\n",
    "    indices_x = np.array(indices)\n",
    "    segments = np.array(segments)\n",
    "    return [indices_x, segments]\n",
    "\n",
    "def load_pred_data(question, doc):\n",
    "    data_x = convert_pred_data(question, doc)\n",
    "    return data_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_letter(question, doc):\n",
    "\n",
    "    test_input = load_pred_data(question, doc)             # question과 paragraph를 token,segment로 변수에 저장\n",
    "    test_start, test_end = bert_model.predict(test_input)  # 학습 모델에 넣고 answer의 start와 end token 예측   \n",
    "  \n",
    "    indexes = tokenizer.encode(question, doc, max_len=SEQ_LEN)[0] \n",
    "    start = np.argmax(test_start, axis=1).item()      # 예측한 start_token의 위치 \n",
    "    end = np.argmax(test_end, axis=1).item()          # 예측한 end_token의 위치\n",
    "    start_tok = indexes[start]                        # 예측한 start_token\n",
    "    end_tok = indexes[end]                            # 예측한 end_token\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    for i in range(start, end+1):\n",
    "        token_based_word = reverse_token_dict[indexes[i]]\n",
    "        sentences.append(token_based_word)\n",
    "        #print(token_based_word, end= \" \")                   # 예측한 정답, start와 end 토큰 사이의 모든 토큰을 보여줌\n",
    "    \n",
    "    answer = []\n",
    "    for w in sentences:\n",
    "        if w.startswith(\"##\"):\n",
    "            w = w.replace(\"##\", \"\")\n",
    "        else:\n",
    "            w = \" \" + w                                 # 예측한 정답의 ##를 제외하고 보여준다.\n",
    "        \n",
    "        answer.append(w)\n",
    "    \n",
    "    predict=\"\".join(answer)\n",
    "    \n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 플라톤'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = '고대 그리스 수학자는 플라톤이다.'\n",
    "question = \"수학자는 누구인가?\"\n",
    "\n",
    "predict_letter(question, doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_letter2(inputs):\n",
    "    doc, question =inputs.split(',')\n",
    "    test_input = load_pred_data(question, doc)             # question과 paragraph를 token,segment로 변수에 저장\n",
    "    test_start, test_end = bert_model.predict(test_input)  # 학습 모델에 넣고 answer의 start와 end token 예측   \n",
    "  \n",
    "    indexes = tokenizer.encode(question, doc, max_len=SEQ_LEN)[0] \n",
    "    start = np.argmax(test_start, axis=1).item()      # 예측한 start_token의 위치 \n",
    "    end = np.argmax(test_end, axis=1).item()          # 예측한 end_token의 위치\n",
    "    start_tok = indexes[start]                        # 예측한 start_token\n",
    "    end_tok = indexes[end]                            # 예측한 end_token\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    for i in range(start, end+1):\n",
    "        token_based_word = reverse_token_dict[indexes[i]]\n",
    "        sentences.append(token_based_word)\n",
    "        #print(token_based_word, end= \" \")                   # 예측한 정답, start와 end 토큰 사이의 모든 토큰을 보여줌\n",
    "    \n",
    "    answer = []\n",
    "    for w in sentences:\n",
    "        if w.startswith(\"##\"):\n",
    "            w = w.replace(\"##\", \"\")\n",
    "        else:\n",
    "            w = \" \" + w                                 # 예측한 정답의 ##를 제외하고 보여준다.\n",
    "        \n",
    "        answer.append(w)\n",
    "    \n",
    "    predict=\"\".join(answer)\n",
    "    \n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 플라톤'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_letter2('고대 그리스 수학자는 플라톤이다., 수학자는 누구인가?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 불'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_letter2('정다면체는 한 꼭짓점에 들어가는 면의 개수가 같다. 그리고 결정적으로 볼록하다. 서양에서는 플라톤 다면체란 말로 별 정다면체와 구분하고 있다. 모든 정다면체의 개수는 5개이다. 유클리드는 각각의 다면체가 하나의 원소를 나타낸다고 생각했다. 정사면체는 불을 나타낸다. 정육면체는 흙을 나타낸다. 정팔면체는 공기를 나타낸다. 정이십면체는 물을 나타낸다. 그리고 정십이면체는 우주를 나타낸다., 정사면체가 의미하는 것은?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
